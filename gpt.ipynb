{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import mmap\n",
    "import random\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='This is a demonstration program')\n",
    "\n",
    "# batch_size = args.batch_size # to use the batch_size cmd arg -> python file_name.py -batch_size 32\n",
    "batch_size = 32\n",
    "block_size = 128\n",
    "max_iters = 200\n",
    "learning_rate = 2e-5\n",
    "eval_iters = 1\n",
    "n_embd = 384\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wizard_of_oz.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_int = { ch:i for i,ch in enumerate(chars) }\n",
    "int_to_string = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.8*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "# [1, 0, 0]\n",
    "# [1, 0.6, 0]\n",
    "# [1, 0.6, 0.4]\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # (B, T, F) -> (B, T, [h1, h1, h1, h1, h2, h2, h2, h2, h3, h3, h3, h3])\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "    \n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.sa(x)\n",
    "        x = self.ln1(x + y)\n",
    "        y = self.ffwd(x)\n",
    "        x = self.ln2(x + y)\n",
    "        return x\n",
    "    \n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        B, T = index.shape\n",
    "        \n",
    "        \n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(index) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, index, max_new_tokens):\n",
    "        # index is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            index_cond = index[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self.forward(index_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            index = torch.cat((index, index_next), dim=1) # (B, T+1)\n",
    "        return index\n",
    "\n",
    "model = GPTLanguageModel(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 4.349, val loss: 4.357\n",
      "step: 1, train loss: 4.299, val loss: 4.314\n",
      "step: 2, train loss: 4.268, val loss: 4.265\n",
      "step: 3, train loss: 4.211, val loss: 4.207\n",
      "step: 4, train loss: 4.167, val loss: 4.161\n",
      "step: 5, train loss: 4.114, val loss: 4.113\n",
      "step: 6, train loss: 4.084, val loss: 4.068\n",
      "step: 7, train loss: 4.022, val loss: 4.033\n",
      "step: 8, train loss: 3.980, val loss: 3.982\n",
      "step: 9, train loss: 3.954, val loss: 3.945\n",
      "step: 10, train loss: 3.893, val loss: 3.897\n",
      "step: 11, train loss: 3.865, val loss: 3.851\n",
      "step: 12, train loss: 3.832, val loss: 3.828\n",
      "step: 13, train loss: 3.772, val loss: 3.786\n",
      "step: 14, train loss: 3.748, val loss: 3.754\n",
      "step: 15, train loss: 3.701, val loss: 3.709\n",
      "step: 16, train loss: 3.656, val loss: 3.662\n",
      "step: 17, train loss: 3.623, val loss: 3.642\n",
      "step: 18, train loss: 3.584, val loss: 3.593\n",
      "step: 19, train loss: 3.557, val loss: 3.578\n",
      "step: 20, train loss: 3.531, val loss: 3.551\n",
      "step: 21, train loss: 3.488, val loss: 3.513\n",
      "step: 22, train loss: 3.483, val loss: 3.493\n",
      "step: 23, train loss: 3.455, val loss: 3.479\n",
      "step: 24, train loss: 3.441, val loss: 3.432\n",
      "step: 25, train loss: 3.426, val loss: 3.416\n",
      "step: 26, train loss: 3.388, val loss: 3.394\n",
      "step: 27, train loss: 3.357, val loss: 3.364\n",
      "step: 28, train loss: 3.325, val loss: 3.353\n",
      "step: 29, train loss: 3.341, val loss: 3.289\n",
      "step: 30, train loss: 3.269, val loss: 3.302\n",
      "step: 31, train loss: 3.295, val loss: 3.316\n",
      "step: 32, train loss: 3.280, val loss: 3.251\n",
      "step: 33, train loss: 3.239, val loss: 3.249\n",
      "step: 34, train loss: 3.232, val loss: 3.234\n",
      "step: 35, train loss: 3.178, val loss: 3.216\n",
      "step: 36, train loss: 3.197, val loss: 3.216\n",
      "step: 37, train loss: 3.197, val loss: 3.219\n",
      "step: 38, train loss: 3.178, val loss: 3.185\n",
      "step: 39, train loss: 3.147, val loss: 3.181\n",
      "step: 40, train loss: 3.180, val loss: 3.140\n",
      "step: 41, train loss: 3.142, val loss: 3.131\n",
      "step: 42, train loss: 3.151, val loss: 3.154\n",
      "step: 43, train loss: 3.140, val loss: 3.144\n",
      "step: 44, train loss: 3.125, val loss: 3.138\n",
      "step: 45, train loss: 3.129, val loss: 3.102\n",
      "step: 46, train loss: 3.071, val loss: 3.109\n",
      "step: 47, train loss: 3.106, val loss: 3.097\n",
      "step: 48, train loss: 3.098, val loss: 3.056\n",
      "step: 49, train loss: 3.080, val loss: 3.067\n",
      "step: 50, train loss: 3.048, val loss: 3.086\n",
      "step: 51, train loss: 3.044, val loss: 3.064\n",
      "step: 52, train loss: 3.045, val loss: 3.063\n",
      "step: 53, train loss: 3.012, val loss: 3.031\n",
      "step: 54, train loss: 3.032, val loss: 3.033\n",
      "step: 55, train loss: 3.024, val loss: 3.047\n",
      "step: 56, train loss: 3.019, val loss: 3.033\n",
      "step: 57, train loss: 3.006, val loss: 3.004\n",
      "step: 58, train loss: 3.027, val loss: 3.040\n",
      "step: 59, train loss: 2.986, val loss: 3.024\n",
      "step: 60, train loss: 2.991, val loss: 2.994\n",
      "step: 61, train loss: 3.035, val loss: 2.989\n",
      "step: 62, train loss: 2.996, val loss: 2.986\n",
      "step: 63, train loss: 2.964, val loss: 3.013\n",
      "step: 64, train loss: 2.967, val loss: 3.006\n",
      "step: 65, train loss: 2.936, val loss: 2.957\n",
      "step: 66, train loss: 2.938, val loss: 2.960\n",
      "step: 67, train loss: 2.942, val loss: 2.976\n",
      "step: 68, train loss: 2.942, val loss: 2.924\n",
      "step: 69, train loss: 2.952, val loss: 2.943\n",
      "step: 70, train loss: 2.931, val loss: 2.982\n",
      "step: 71, train loss: 2.934, val loss: 2.974\n",
      "step: 72, train loss: 2.902, val loss: 2.969\n",
      "step: 73, train loss: 2.888, val loss: 2.918\n",
      "step: 74, train loss: 2.905, val loss: 2.915\n",
      "step: 75, train loss: 2.893, val loss: 2.944\n",
      "step: 76, train loss: 2.903, val loss: 2.908\n",
      "step: 77, train loss: 2.908, val loss: 2.942\n",
      "step: 78, train loss: 2.903, val loss: 2.910\n",
      "step: 79, train loss: 2.888, val loss: 2.890\n",
      "step: 80, train loss: 2.877, val loss: 2.887\n",
      "step: 81, train loss: 2.881, val loss: 2.900\n",
      "step: 82, train loss: 2.877, val loss: 2.888\n",
      "step: 83, train loss: 2.846, val loss: 2.901\n",
      "step: 84, train loss: 2.868, val loss: 2.920\n",
      "step: 85, train loss: 2.868, val loss: 2.843\n",
      "step: 86, train loss: 2.886, val loss: 2.901\n",
      "step: 87, train loss: 2.863, val loss: 2.869\n",
      "step: 88, train loss: 2.838, val loss: 2.905\n",
      "step: 89, train loss: 2.843, val loss: 2.859\n",
      "step: 90, train loss: 2.823, val loss: 2.832\n",
      "step: 91, train loss: 2.827, val loss: 2.858\n",
      "step: 92, train loss: 2.814, val loss: 2.861\n",
      "step: 93, train loss: 2.803, val loss: 2.838\n",
      "step: 94, train loss: 2.814, val loss: 2.793\n",
      "step: 95, train loss: 2.792, val loss: 2.809\n",
      "step: 96, train loss: 2.801, val loss: 2.788\n",
      "step: 97, train loss: 2.825, val loss: 2.831\n",
      "step: 98, train loss: 2.793, val loss: 2.848\n",
      "step: 99, train loss: 2.813, val loss: 2.789\n",
      "step: 100, train loss: 2.791, val loss: 2.820\n",
      "step: 101, train loss: 2.788, val loss: 2.790\n",
      "step: 102, train loss: 2.779, val loss: 2.796\n",
      "step: 103, train loss: 2.811, val loss: 2.814\n",
      "step: 104, train loss: 2.788, val loss: 2.769\n",
      "step: 105, train loss: 2.742, val loss: 2.769\n",
      "step: 106, train loss: 2.764, val loss: 2.757\n",
      "step: 107, train loss: 2.830, val loss: 2.754\n",
      "step: 108, train loss: 2.760, val loss: 2.796\n",
      "step: 109, train loss: 2.768, val loss: 2.810\n",
      "step: 110, train loss: 2.769, val loss: 2.746\n",
      "step: 111, train loss: 2.793, val loss: 2.755\n",
      "step: 112, train loss: 2.774, val loss: 2.774\n",
      "step: 113, train loss: 2.775, val loss: 2.753\n",
      "step: 114, train loss: 2.739, val loss: 2.771\n",
      "step: 115, train loss: 2.777, val loss: 2.767\n",
      "step: 116, train loss: 2.713, val loss: 2.771\n",
      "step: 117, train loss: 2.739, val loss: 2.767\n",
      "step: 118, train loss: 2.717, val loss: 2.749\n",
      "step: 119, train loss: 2.736, val loss: 2.743\n",
      "step: 120, train loss: 2.713, val loss: 2.724\n",
      "step: 121, train loss: 2.716, val loss: 2.728\n",
      "step: 122, train loss: 2.704, val loss: 2.724\n",
      "step: 123, train loss: 2.754, val loss: 2.737\n",
      "step: 124, train loss: 2.685, val loss: 2.733\n",
      "step: 125, train loss: 2.731, val loss: 2.718\n",
      "step: 126, train loss: 2.740, val loss: 2.726\n",
      "step: 127, train loss: 2.675, val loss: 2.719\n",
      "step: 128, train loss: 2.727, val loss: 2.688\n",
      "step: 129, train loss: 2.691, val loss: 2.696\n",
      "step: 130, train loss: 2.692, val loss: 2.740\n",
      "step: 131, train loss: 2.673, val loss: 2.688\n",
      "step: 132, train loss: 2.700, val loss: 2.675\n",
      "step: 133, train loss: 2.665, val loss: 2.738\n",
      "step: 134, train loss: 2.689, val loss: 2.659\n",
      "step: 135, train loss: 2.708, val loss: 2.704\n",
      "step: 136, train loss: 2.714, val loss: 2.683\n",
      "step: 137, train loss: 2.716, val loss: 2.691\n",
      "step: 138, train loss: 2.656, val loss: 2.662\n",
      "step: 139, train loss: 2.695, val loss: 2.676\n",
      "step: 140, train loss: 2.713, val loss: 2.675\n",
      "step: 141, train loss: 2.639, val loss: 2.706\n",
      "step: 142, train loss: 2.655, val loss: 2.674\n",
      "step: 143, train loss: 2.664, val loss: 2.697\n",
      "step: 144, train loss: 2.653, val loss: 2.678\n",
      "step: 145, train loss: 2.718, val loss: 2.685\n",
      "step: 146, train loss: 2.672, val loss: 2.665\n",
      "step: 147, train loss: 2.669, val loss: 2.678\n",
      "step: 148, train loss: 2.672, val loss: 2.683\n",
      "step: 149, train loss: 2.686, val loss: 2.670\n",
      "step: 150, train loss: 2.699, val loss: 2.608\n",
      "step: 151, train loss: 2.628, val loss: 2.640\n",
      "step: 152, train loss: 2.649, val loss: 2.673\n",
      "step: 153, train loss: 2.647, val loss: 2.675\n",
      "step: 154, train loss: 2.647, val loss: 2.637\n",
      "step: 155, train loss: 2.640, val loss: 2.659\n",
      "step: 156, train loss: 2.649, val loss: 2.627\n",
      "step: 157, train loss: 2.653, val loss: 2.636\n",
      "step: 158, train loss: 2.611, val loss: 2.670\n",
      "step: 159, train loss: 2.665, val loss: 2.644\n",
      "step: 160, train loss: 2.634, val loss: 2.655\n",
      "step: 161, train loss: 2.645, val loss: 2.616\n",
      "step: 162, train loss: 2.620, val loss: 2.636\n",
      "step: 163, train loss: 2.623, val loss: 2.643\n",
      "step: 164, train loss: 2.610, val loss: 2.651\n",
      "step: 165, train loss: 2.612, val loss: 2.628\n",
      "step: 166, train loss: 2.647, val loss: 2.629\n",
      "step: 167, train loss: 2.611, val loss: 2.620\n",
      "step: 168, train loss: 2.628, val loss: 2.662\n",
      "step: 169, train loss: 2.623, val loss: 2.643\n",
      "step: 170, train loss: 2.605, val loss: 2.599\n",
      "step: 171, train loss: 2.632, val loss: 2.629\n",
      "step: 172, train loss: 2.606, val loss: 2.584\n",
      "step: 173, train loss: 2.590, val loss: 2.601\n",
      "step: 174, train loss: 2.638, val loss: 2.626\n",
      "step: 175, train loss: 2.604, val loss: 2.588\n",
      "step: 176, train loss: 2.615, val loss: 2.593\n",
      "step: 177, train loss: 2.581, val loss: 2.602\n",
      "step: 178, train loss: 2.604, val loss: 2.588\n",
      "step: 179, train loss: 2.596, val loss: 2.598\n",
      "step: 180, train loss: 2.650, val loss: 2.643\n",
      "step: 181, train loss: 2.565, val loss: 2.569\n",
      "step: 182, train loss: 2.675, val loss: 2.603\n",
      "step: 183, train loss: 2.564, val loss: 2.584\n",
      "step: 184, train loss: 2.606, val loss: 2.613\n",
      "step: 185, train loss: 2.583, val loss: 2.550\n",
      "step: 186, train loss: 2.555, val loss: 2.587\n",
      "step: 187, train loss: 2.607, val loss: 2.620\n",
      "step: 188, train loss: 2.579, val loss: 2.589\n",
      "step: 189, train loss: 2.589, val loss: 2.533\n",
      "step: 190, train loss: 2.560, val loss: 2.594\n",
      "step: 191, train loss: 2.577, val loss: 2.590\n",
      "step: 192, train loss: 2.556, val loss: 2.540\n",
      "step: 193, train loss: 2.593, val loss: 2.599\n",
      "step: 194, train loss: 2.570, val loss: 2.574\n",
      "step: 195, train loss: 2.535, val loss: 2.612\n",
      "step: 196, train loss: 2.563, val loss: 2.601\n",
      "step: 197, train loss: 2.547, val loss: 2.585\n",
      "step: 198, train loss: 2.596, val loss: 2.589\n",
      "step: 199, train loss: 2.565, val loss: 2.595\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step: {iter}, train loss: {losses['train']:.3f}, val loss: {losses['val']:.3f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! My name ishaFo1e L﻿e )iKaI m,s me “f,orry Ue GtfTm ut fd tGofcp,, ban hasgfowureade H“he-thelretanghe wiy Dino\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Hello! My name is\"\n",
    "context = torch.tensor(encode(prompt), dtype=torch.long)\n",
    "generated_chars = decode(model.generate(context.unsqueeze(0), max_new_tokens=100)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
